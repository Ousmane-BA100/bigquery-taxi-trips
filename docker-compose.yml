version: '3.8'

services:
  airflow:
    image: apache/airflow:2.7.3-python3.10
    container_name: airflow
    restart: always
    depends_on:
      - custom_spark_airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=your_fernet_key_here
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./tmp/dbt_credentials:/tmp/dbt_credentials:ro
    ports:
      - "8080:8080"
    command: bash -c "airflow db init && airflow webserver & airflow scheduler"

  custom_spark_airflow:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: custom_spark_airflow
    volumes:
      - .:/app
    environment:
      - SPARK_HOME=/opt/spark
      - PYSPARK_PYTHON=python3
    tty: true
    stdin_open: true

volumes:
  airflow_logs:
  airflow_plugins:
